{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d5ef39",
   "metadata": {},
   "source": [
    "# Deeplabv3 Imagenet1k - Validation pre Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a46150cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from cityscapesscripts.helpers import labels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "from backbones_unet.model.unet import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "869aec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set for reproducibility is 42\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "print(f\"Seeds set for reproducibility is {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d6ac1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else \"mps\" if torch.mps.is_available() \n",
    "                      else \"cpu\"\n",
    "                    )\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02748e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, targets_dir, image_transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (string): Directory with all the images.\n",
    "            targets_dir (string): Directory with all the target masks.\n",
    "            image_transform (callable, optional): Optional transform to be applied on images.\n",
    "            target_transform (callable, optional): Optional transform to be applied on targets.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.targets_dir = targets_dir\n",
    "        self.image_transform = image_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Get all image filenames\n",
    "        self.image_filenames = [f for f in os.listdir(images_dir) \n",
    "                               if f.lower().endswith(('.png'))]\n",
    "        self.image_filenames.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load target mask\n",
    "        target_name = img_name.replace('.png', '_trainId.png')\n",
    "        target_path = os.path.join(self.targets_dir, target_name)\n",
    "        target = Image.open(target_path)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "       # else:\n",
    "       #     # Default: convert to tensor\n",
    "       #     target = torch.from_numpy(target)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ecb33",
   "metadata": {},
   "source": [
    "### Loading the Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fed0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = \"syn_resized_images\"\n",
    "path_target = \"syn_resized_gt\"\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 480)), # We augment from 466x256 to 480x256 in order to avoid the padding artifacts\n",
    "    transforms.ToTensor(),  # Converts PIL Image to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize with ImageNet parameters\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 480), interpolation=Image.NEAREST), # This interpolation ensure that all pixels have a correct value of their class\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(np.array(x)).long())\n",
    "])\n",
    "syn_dataset = SegmentationDataset(images_dir=path_images, targets_dir=path_target, image_transform=image_transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b133d44",
   "metadata": {},
   "source": [
    "### Splitting Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfdec182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 12500\n",
      "Train size: 7500 (60.0%)\n",
      "Validation size: 1250 (10.0%)\n",
      "Test size: 3750 (30.0%)\n",
      "\n",
      "DataLoaders created:\n",
      "Train batches: 938\n",
      "Validation batches: 157\n",
      "Test batches: 469\n"
     ]
    }
   ],
   "source": [
    "# Get total dataset size\n",
    "total_size = len(syn_dataset)\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "\n",
    "# Calculate split sizes (60% train, 10% val, 30% test)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "print(f\"Train size: {train_size} ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"Validation size: {val_size} ({val_size/total_size*100:.1f}%)\")\n",
    "print(f\"Test size: {test_size} ({test_size/total_size*100:.1f}%)\")\n",
    "\n",
    "# Create random splits\n",
    "syn_train_dataset, syn_val_dataset, syn_test_dataset = random_split(\n",
    "    syn_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)  # For reproducibility\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "\n",
    "syn_train_dataloader = DataLoader(\n",
    "    syn_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    #num_workers=2,\n",
    "    generator=torch.Generator().manual_seed(SEED) \n",
    ")\n",
    "\n",
    "\n",
    "syn_val_dataloader = DataLoader(\n",
    "    syn_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  \n",
    "    #num_workers=2,\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "syn_test_dataloader = DataLoader(\n",
    "    syn_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  \n",
    "    #num_workers=2,\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"Train batches: {len(syn_train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(syn_val_dataloader)}\")\n",
    "print(f\"Test batches: {len(syn_test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc3060",
   "metadata": {},
   "source": [
    "### Loading the Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "226dce0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 5000\n",
      "Train size: 2975 (59.5%)\n",
      "Validation size: 500 (10.0%)\n",
      "Test size: 1525 (30.5%)\n"
     ]
    }
   ],
   "source": [
    "def cityscapes_mask_to_train_ids(mask):\n",
    "    mask_np = np.array(mask)\n",
    "    labels_mapping = {label.id: label.trainId for label in labels.labels if label.trainId != -1}\n",
    "    labels_mapping[-1] = 255  # Map invalid labels to 255\n",
    "    mask_np = np.vectorize(labels_mapping.get)(mask_np)\n",
    "    mask_np = np.where(mask_np <= 18, mask_np, 255).astype(np.uint8)  # Ensure that all pixels are within the range of train IDs\n",
    "    return torch.from_numpy(mask_np).long()\n",
    "\n",
    "real_target_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 480), interpolation=Image.NEAREST), # This interpolation ensure that all pixels have a correct value of their class\n",
    "    transforms.Lambda(lambda x: cityscapes_mask_to_train_ids(x))  # Convert Cityscapes mask to train IDs\n",
    "])\n",
    "\n",
    "real_train_dataset = Cityscapes(\n",
    "    root='cityscapes',\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=image_transform,\n",
    "    target_transform=real_target_transform\n",
    ")\n",
    "\n",
    "real_val_dataset = Cityscapes(\n",
    "    root='cityscapes',\n",
    "    split='val',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=image_transform,\n",
    "    target_transform=real_target_transform\n",
    ")\n",
    "\n",
    "real_test_dataset = Cityscapes(\n",
    "    root='cityscapes',\n",
    "    split='test',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    transform=image_transform,\n",
    "    target_transform=real_target_transform\n",
    ")\n",
    "\n",
    "# Print dataloaders sizes\n",
    "real_total_size = len(real_train_dataset) + len(real_val_dataset) + len(real_test_dataset)\n",
    "print(f\"Total dataset size: {real_total_size}\")\n",
    "print(f\"Train size: {len(real_train_dataset)} ({len(real_train_dataset)/real_total_size*100:.1f}%)\")\n",
    "print(f\"Validation size: {len(real_val_dataset)} ({len(real_val_dataset)/real_total_size*100:.1f}%)\")\n",
    "print(f\"Test size: {len(real_test_dataset)} ({len(real_test_dataset)/real_total_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a35e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_dataloader = DataLoader(\n",
    "    real_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    #num_workers=2,\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "real_val_dataloader = DataLoader(\n",
    "    real_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  \n",
    "    #num_workers=2,\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "real_test_dataloader = DataLoader(\n",
    "    real_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  \n",
    "    #num_workers=2,\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d37ef0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=0.1, ignore_index=None):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: (N, C, H, W) - logits (non softmaxati)\n",
    "        targets: (N, H, W)   - ground truth con classi (0...C-1)\n",
    "        \"\"\"\n",
    "        num_classes = inputs.shape[1]\n",
    "        device = inputs.device  # Get device from input tensor\n",
    "        \n",
    "        # Softmax sulle predizioni\n",
    "        probs = F.softmax(inputs, dim=1)  # (N, C, H, W)\n",
    "        \n",
    "        # Handle ignore_index by creating a mask and filtering out ignored pixels\n",
    "        if self.ignore_index is not None:\n",
    "            # Create mask for valid pixels\n",
    "            valid_mask = (targets != self.ignore_index)  # (N, H, W)\n",
    "            \n",
    "            # Only process valid pixels\n",
    "            valid_targets = targets[valid_mask]  # (N_valid,)\n",
    "            \n",
    "            # Reshape probs to match and filter valid pixels\n",
    "            probs_reshaped = probs.permute(0, 2, 3, 1)  # (N, H, W, C)\n",
    "            valid_probs = probs_reshaped[valid_mask]  # (N_valid, C)\n",
    "            \n",
    "        else:\n",
    "            # No ignore_index, process all pixels\n",
    "            valid_targets = targets.view(-1)  # (N*H*W,)\n",
    "            valid_probs = probs.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)  # (N*H*W, C)\n",
    "        \n",
    "        # One-hot encoding of valid targets only\n",
    "        if len(valid_targets) > 0:\n",
    "            targets_one_hot = F.one_hot(valid_targets, num_classes=num_classes).float()  # (N_valid, C)\n",
    "        else:\n",
    "            # If no valid pixels, create a zero loss that maintains gradients\n",
    "            # Use a small operation on the input to maintain gradient flow\n",
    "            zero_loss = (inputs * 0.0).sum()  # This maintains gradients from inputs\n",
    "            return zero_loss\n",
    "        \n",
    "        # Calcolo Dice per ogni classe usando solo pixel validi\n",
    "        intersection = (valid_probs * targets_one_hot).sum(dim=0)  # (C,)\n",
    "        union = valid_probs.sum(dim=0) + targets_one_hot.sum(dim=0)  # (C,)\n",
    "        \n",
    "        \n",
    "        smooth_tensor = torch.tensor(self.smooth, device=device, dtype=intersection.dtype)\n",
    "        dice = (2.0 * intersection + smooth_tensor) / (union + smooth_tensor)\n",
    "        \n",
    "        # Media sulle classi\n",
    "        loss = 1.0 - dice.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9329fb",
   "metadata": {},
   "source": [
    "## Maximum Mean Discrepancy Loss (MMD) \n",
    "$$\n",
    "    MMD(X, Y) = \\frac{1}{|X|^2} \\sum_{i,j} k(x_i, x_j) - \\frac{2}{|X||Y|} \\sum_{i,j} k(x_i, y_j) + \\frac{1}{|Y|^2} \\sum_{i,j} k(y_i, y_j)\n",
    "$$\n",
    "where:\n",
    "- $k$ is a kernel function (e.g., Gaussian).\n",
    "- $X$ is the source domain (MNIST) and $Y$ is the target domain (SVHN).\n",
    "- $x_i$ and $y_j$ are samples from the respective domains.\n",
    "- $|X|$ and $|Y|$ are the number of samples in each domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b879bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MMD loss between source and target feature batches\n",
    "def rbf_kernel(a, b, sigma):\n",
    "    a = a.unsqueeze(1) \n",
    "    b = b.unsqueeze(0) \n",
    "    dist = (a - b).pow(2).sum(2) \n",
    "    return torch.exp(-dist / (2 * sigma ** 2))\n",
    "\n",
    "def MMD_loss(x, y, sigma=1.0):\n",
    "\n",
    "    K_xx = rbf_kernel(x, x, sigma)\n",
    "    K_yy = rbf_kernel(y, y, sigma)\n",
    "    K_xy = rbf_kernel(x, y, sigma)\n",
    "    mmd = K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()\n",
    "    return mmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26b66662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import urllib.request\n",
    "from contextlib import contextmanager\n",
    "# Disable SSL verification for urllib requests on MacOS\n",
    "# This is a workaround for the \"SSL: CERTIFICATE_VERIFY_FAILED\" error on MacOS\n",
    "@contextmanager\n",
    "def no_ssl_verification():\n",
    "    \"\"\"Temporarily disable SSL verification\"\"\"\n",
    "    old_context = ssl._create_default_https_context\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        ssl._create_default_https_context = old_context\n",
    "\n",
    "NUM_CLASSES = 19\n",
    "LR = 1e-4\n",
    "\n",
    "CE_loss = nn.CrossEntropyLoss(ignore_index=255)  # 255 = unlabeled\n",
    "Dice_loss = DiceLoss(smooth=0.1, ignore_index=255)  \n",
    "\n",
    "ce_importance = 0.7\n",
    "mmd_weight = 0.1\n",
    "\n",
    "def criterion(outputs, targets):\n",
    "    ce_loss = CE_loss(outputs, targets)\n",
    "    dice_loss = Dice_loss(outputs, targets)\n",
    "\n",
    "    cls_loss = ce_importance * ce_loss + (1 - ce_importance) * dice_loss\n",
    "    return cls_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64e36576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps detected, using no_ssl_verification\n",
      "Loading model from: models/unet_imagenet1k_480x256_best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "# Instantiate the model architecture\n",
    "if DEVICE.type == 'mps': \n",
    "    print(\"mps detected, using no_ssl_verification\")\n",
    "    with no_ssl_verification():\n",
    "        model = Unet(\n",
    "            backbone='resnet50',\n",
    "            pretrained=True,\n",
    "            in_channels=3,\n",
    "            num_classes=NUM_CLASSES,\n",
    "        )\n",
    "else:\n",
    "    model = Unet(\n",
    "        backbone='resnet50',\n",
    "        pretrained=True,\n",
    "        in_channels=3,\n",
    "        num_classes=NUM_CLASSES,\n",
    "    )\n",
    "    \n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Load the checkpoint and filter out aux_classifier keys\n",
    "best_model_path = \"models/unet_imagenet1k_480x256_best_model.pth\"\n",
    "checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "print (f\"Loading model from: {best_model_path}\")\n",
    "\n",
    "name = \"DA_unet_imagenet1k_480x256\"\n",
    "\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a83b9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(preds, labels, num_classes, ignore_index=255):\n",
    "    \n",
    "    preds = torch.argmax(preds, dim=1).detach().cpu()  # [B, H, W]\n",
    "    \n",
    "    labels = labels.detach().cpu() \n",
    "    \n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (labels == cls)\n",
    "        \n",
    "        # Escludi pixel ignorati\n",
    "        mask = (labels != ignore_index)\n",
    "        pred_inds = pred_inds & mask\n",
    "        target_inds = target_inds & mask\n",
    "\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        \n",
    "        if union == 0:\n",
    "            continue  # salta classe non presente\n",
    "        ious.append(intersection / union)\n",
    "    \n",
    "    if len(ious) == 0:\n",
    "        return float('nan')  # o 0.0 se preferisci\n",
    "    return sum(ious) / len(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d5bea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Clear memory and cache for all device types\"\"\"\n",
    "    # Run garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Get device from global scope\n",
    "    if 'DEVICE' in globals():\n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if DEVICE.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    # Second GC run to make sure everything is cleaned up\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c10edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(\n",
    "        model, criterion, syn_val_dataloader, real_val_dataloader\n",
    "    ):\n",
    "    \n",
    "    # Validation \n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    val_iou_source = 0\n",
    "    val_iou_target = 0\n",
    "\n",
    "    val_steps = min(len(syn_val_dataloader), len(real_val_dataloader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        syn_val_iter = iter(syn_val_dataloader)\n",
    "        real_val_iter = iter(real_val_dataloader)\n",
    "\n",
    "        for i in tqdm(range(val_steps), desc=\"Validation\"):\n",
    "            source_data, source_labels = next(syn_val_iter)\n",
    "            target_data, target_labels = next(real_val_iter)\n",
    "            source_data, source_labels = source_data.to(DEVICE), source_labels.to(DEVICE)\n",
    "            target_data, target_labels = target_data.to(DEVICE), target_labels.to(DEVICE)\n",
    "\n",
    "            outputs_source = model(source_data)\n",
    "            outputs_target = model(target_data)\n",
    "\n",
    "            loss_real = criterion(outputs_source, source_labels)\n",
    "            loss_target = criterion(outputs_target, target_labels)\n",
    "\n",
    "            val_total_loss += loss_real.detach().cpu().item() \n",
    "            val_iou_source += compute_iou(outputs_source.detach().cpu(), source_labels, NUM_CLASSES)\n",
    "            val_iou_target += compute_iou(outputs_target.detach().cpu(), target_labels, NUM_CLASSES)\n",
    "\n",
    "            # Explicit cleanup\n",
    "            del source_data, source_labels, target_data, target_labels\n",
    "            del outputs_source, outputs_target, loss_real, loss_target\n",
    "            \n",
    "            # Periodic memory clear\n",
    "            if (i + 1) % 5 == 0:\n",
    "                clear_memory()\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    # Average losses and IOUs\n",
    "    source_val_avg_loss = val_total_loss / val_steps\n",
    "    source_val_avg_iou = val_iou_source / val_steps\n",
    "    target_val_avg_iou = val_iou_target / val_steps\n",
    "    \n",
    "    return source_val_avg_loss, source_val_avg_iou, target_val_avg_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32339b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded successfully!\n",
      "Best validation IoU: 0.5983\n",
      "From epoch: 24\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model loaded successfully!\")\n",
    "print(f\"Best validation IoU: {checkpoint['val_iou']:.4f}\")\n",
    "print(f\"From epoch: {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "249e8679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 63/63 [01:06<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Validation Average Loss: 0.2589\n",
      "Source Validation Average IoU: 0.5927\n",
      "Target Validation Average IoU: 0.2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_val_avg_loss, source_val_avg_iou, target_val_avg_iou = validate_epoch(\n",
    "    model, criterion, syn_val_dataloader, real_val_dataloader\n",
    ")\n",
    "\n",
    "print(f\"Source Validation Average Loss: {source_val_avg_loss:.4f}\")\n",
    "print(f\"Source Validation Average IoU: {source_val_avg_iou:.4f}\")\n",
    "print(f\"Target Validation Average IoU: {target_val_avg_iou:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
